"""Feature transformers for ML preprocessing.

This module provides scikit-learn compatible transformers for preprocessing
features generated by the feature generators. Includes scaling, imputation,
and feature selection utilities.
"""

from pathlib import Path
from typing import Any

import joblib
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


class FeatureScaler(BaseEstimator, TransformerMixin):  # type: ignore[misc]
    """Standard scaler with missing value handling.

    Scales features to zero mean and unit variance, handling missing
    values by imputation before scaling.
    """

    def __init__(
        self,
        with_mean: bool = True,
        with_std: bool = True,
        impute_strategy: str = "median",
    ) -> None:
        """Initialize feature scaler.

        Args:
            with_mean: Whether to center data
            with_std: Whether to scale to unit variance
            impute_strategy: Strategy for imputing missing values
                ('mean', 'median', 'most_frequent', 'constant')
        """
        self.with_mean = with_mean
        self.with_std = with_std
        self.impute_strategy = impute_strategy
        self._imputer: SimpleImputer | None = None
        self._scaler: StandardScaler | None = None
        self._feature_names: list[str] | None = None

    def fit(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> "FeatureScaler":
        """Fit the scaler on training data.

        Args:
            X: Feature matrix (samples x features)
            y: Ignored, present for API compatibility

        Returns:
            self
        """
        if isinstance(X, pd.DataFrame):
            self._feature_names = list(X.columns)
            X_array = X.values
        else:
            X_array = X

        # Fit imputer first
        self._imputer = SimpleImputer(strategy=self.impute_strategy)
        self._imputer.fit(X_array)

        # Transform and fit scaler
        X_imputed = self._imputer.transform(X_array)
        self._scaler = StandardScaler(
            with_mean=self.with_mean,
            with_std=self.with_std,
        )
        self._scaler.fit(X_imputed)

        return self

    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Transform data using fitted scaler.

        Args:
            X: Feature matrix to transform

        Returns:
            Scaled feature matrix
        """
        if self._imputer is None or self._scaler is None:
            raise ValueError("Scaler not fitted. Call fit() first.")

        X_array = X.values if isinstance(X, pd.DataFrame) else X

        X_imputed = self._imputer.transform(X_array)
        return self._scaler.transform(X_imputed)

    def fit_transform(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(X, y)
        return self.transform(X)

    def inverse_transform(self, X: np.ndarray) -> np.ndarray:
        """Inverse transform scaled data back to original scale.

        Args:
            X: Scaled feature matrix

        Returns:
            Feature matrix in original scale
        """
        if self._scaler is None:
            raise ValueError("Scaler not fitted. Call fit() first.")

        return self._scaler.inverse_transform(X)

    def get_feature_names(self) -> list[str] | None:
        """Get feature names if available."""
        return self._feature_names

    def get_params_dict(self) -> dict[str, Any]:
        """Get parameters for serialization."""
        return {
            "with_mean": self.with_mean,
            "with_std": self.with_std,
            "impute_strategy": self.impute_strategy,
            "feature_names": self._feature_names,
        }

    def save(self, path: Path) -> None:
        """Save scaler to disk.

        Args:
            path: Path to save scaler
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(
            {
                "imputer": self._imputer,
                "scaler": self._scaler,
                "feature_names": self._feature_names,
                "params": self.get_params_dict(),
            },
            path,
        )

    @classmethod
    def load(cls, path: Path) -> "FeatureScaler":
        """Load scaler from disk.

        Args:
            path: Path to load scaler from

        Returns:
            Loaded FeatureScaler instance
        """
        data = joblib.load(path)
        scaler = cls(
            with_mean=data["params"]["with_mean"],
            with_std=data["params"]["with_std"],
            impute_strategy=data["params"]["impute_strategy"],
        )
        scaler._imputer = data["imputer"]
        scaler._scaler = data["scaler"]
        scaler._feature_names = data["feature_names"]
        return scaler


class MissingValueHandler(BaseEstimator, TransformerMixin):  # type: ignore[misc]
    """Handle missing values with configurable strategies.

    Provides flexible missing value imputation with support for
    different strategies per feature type.
    """

    def __init__(
        self,
        numeric_strategy: str = "median",
        fill_value: float | None = None,
        add_indicator: bool = False,
    ) -> None:
        """Initialize missing value handler.

        Args:
            numeric_strategy: Strategy for numeric features
                ('mean', 'median', 'most_frequent', 'constant')
            fill_value: Value to use when strategy is 'constant'
            add_indicator: Add binary indicators for missing values
        """
        self.numeric_strategy = numeric_strategy
        self.fill_value = fill_value
        self.add_indicator = add_indicator
        self._imputer: SimpleImputer | None = None
        self._feature_names: list[str] | None = None
        self._n_missing_features: int = 0

    def fit(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> "MissingValueHandler":
        """Fit the imputer on training data.

        Args:
            X: Feature matrix
            y: Ignored

        Returns:
            self
        """
        if isinstance(X, pd.DataFrame):
            self._feature_names = list(X.columns)
            X_array = X.values
        else:
            X_array = X

        self._imputer = SimpleImputer(
            strategy=self.numeric_strategy,
            fill_value=self.fill_value,
            add_indicator=self.add_indicator,
        )
        self._imputer.fit(X_array)

        if self.add_indicator:
            self._n_missing_features = (
                len(self._imputer.indicator_.features_)
                if hasattr(self._imputer, "indicator_")
                else 0
            )

        return self

    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Transform data by imputing missing values.

        Args:
            X: Feature matrix

        Returns:
            Imputed feature matrix
        """
        if self._imputer is None:
            raise ValueError("Imputer not fitted. Call fit() first.")

        X_array = X.values if isinstance(X, pd.DataFrame) else X

        return self._imputer.transform(X_array)

    def fit_transform(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(X, y)
        return self.transform(X)

    def get_feature_names(self) -> list[str]:
        """Get feature names including missing indicators if added."""
        if self._feature_names is None:
            return []

        names = list(self._feature_names)
        if self.add_indicator and self._n_missing_features > 0:
            for i in range(self._n_missing_features):
                names.append(f"missing_indicator_{i}")

        return names


class FeatureSelector(BaseEstimator, TransformerMixin):  # type: ignore[misc]
    """Select features based on importance or variance.

    Supports multiple selection strategies for feature subset selection.
    """

    def __init__(
        self,
        selection_type: str = "variance",
        threshold: float = 0.0,
        feature_names: list[str] | None = None,
    ) -> None:
        """Initialize feature selector.

        Args:
            selection_type: Selection strategy
                ('variance', 'importance', 'custom')
            threshold: Threshold for selection
            feature_names: Specific features to select (for 'custom' type)
        """
        self.selection_type = selection_type
        self.threshold = threshold
        self.feature_names = feature_names
        self._selected_indices: np.ndarray | None = None
        self._selected_names: list[str] | None = None
        self._input_feature_names: list[str] | None = None

    def fit(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> "FeatureSelector":
        """Fit selector to determine which features to keep.

        Args:
            X: Feature matrix
            y: Optional target (for importance-based selection)

        Returns:
            self
        """
        if isinstance(X, pd.DataFrame):
            self._input_feature_names = list(X.columns)
            X_array = X.values
        else:
            X_array = X
            self._input_feature_names = [
                f"feature_{i}" for i in range(X_array.shape[1])
            ]

        if self.selection_type == "variance":
            # Select features with variance above threshold
            variances = np.var(X_array, axis=0)
            self._selected_indices = np.where(variances > self.threshold)[0]

        elif self.selection_type == "custom" and self.feature_names:
            # Select specific features by name
            self._selected_indices = np.array(
                [
                    i
                    for i, name in enumerate(self._input_feature_names)
                    if name in self.feature_names
                ]
            )

        else:
            # Keep all features
            self._selected_indices = np.arange(X_array.shape[1])

        self._selected_names = [
            self._input_feature_names[i] for i in self._selected_indices
        ]

        return self

    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Transform data by selecting features.

        Args:
            X: Feature matrix

        Returns:
            Feature matrix with selected features only
        """
        if self._selected_indices is None:
            raise ValueError("Selector not fitted. Call fit() first.")

        X_array = X.values if isinstance(X, pd.DataFrame) else X

        return X_array[:, self._selected_indices]

    def fit_transform(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(X, y)
        return self.transform(X)

    def get_selected_features(self) -> list[str]:
        """Get names of selected features."""
        return self._selected_names or []


class OddsTransformer(BaseEstimator, TransformerMixin):  # type: ignore[misc]
    """Transform betting odds to implied probabilities and derived features.

    Specialized transformer for odds-related feature engineering.
    """

    def __init__(
        self,
        normalize: bool = True,
        add_value_features: bool = True,
    ) -> None:
        """Initialize odds transformer.

        Args:
            normalize: Normalize probabilities to sum to 1
            add_value_features: Add derived value/edge features
        """
        self.normalize = normalize
        self.add_value_features = add_value_features
        self._marginal_stats: dict[str, float] = {}

    def fit(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> "OddsTransformer":
        """Fit transformer to learn typical margin statistics.

        Args:
            X: Feature matrix with odds columns
            y: Ignored

        Returns:
            self
        """
        if isinstance(X, pd.DataFrame) and all(
            c in X.columns
            for c in ["implied_prob_home", "implied_prob_draw", "implied_prob_away"]
        ):
            # Calculate typical margin
            margins = (
                X["implied_prob_home"] + X["implied_prob_draw"] + X["implied_prob_away"]
            )
            self._marginal_stats["mean_margin"] = margins.mean()
            self._marginal_stats["std_margin"] = margins.std()

        return self

    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Transform odds features.

        Args:
            X: Feature matrix

        Returns:
            Transformed feature matrix
        """
        if isinstance(X, pd.DataFrame):
            result = X.copy()

            if self.add_value_features and "implied_prob_home" in result.columns:
                # Add odds-derived value features
                result["odds_balance"] = (
                    result["implied_prob_home"] - result["implied_prob_away"]
                )
                result["draw_likelihood"] = result["implied_prob_draw"]

                # Certainty index (how confident is the market)
                max_prob = result[
                    ["implied_prob_home", "implied_prob_draw", "implied_prob_away"]
                ].max(axis=1)
                result["market_certainty"] = max_prob

            return result.values
        else:
            return X

    def get_feature_names(self, input_features: list[str] | None = None) -> list[str]:
        """Get output feature names."""
        names = list(input_features) if input_features else []
        if self.add_value_features:
            names.extend(["odds_balance", "draw_likelihood", "market_certainty"])
        return names


class TransformerPipeline(BaseEstimator, TransformerMixin):  # type: ignore[misc]
    """Chain multiple transformers into a single pipeline.

    Simplified version of sklearn Pipeline for feature transformation.
    """

    def __init__(self, steps: list[tuple[str, BaseEstimator]]) -> None:
        """Initialize transformer pipeline.

        Args:
            steps: List of (name, transformer) tuples
        """
        self.steps = steps
        self._fitted = False

    def fit(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> "TransformerPipeline":
        """Fit all transformers in sequence.

        Args:
            X: Feature matrix
            y: Optional target

        Returns:
            self
        """
        current_X = X

        for _name, transformer in self.steps:
            if hasattr(transformer, "fit"):
                transformer.fit(current_X, y)
            if hasattr(transformer, "transform"):
                current_X = transformer.transform(current_X)

        self._fitted = True
        return self

    def transform(self, X: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Transform data through all transformers.

        Args:
            X: Feature matrix

        Returns:
            Transformed feature matrix
        """
        if not self._fitted:
            raise ValueError("Pipeline not fitted. Call fit() first.")

        current_X = X

        for _name, transformer in self.steps:
            if hasattr(transformer, "transform"):
                current_X = transformer.transform(current_X)

        return current_X

    def fit_transform(self, X: pd.DataFrame | np.ndarray, y: Any = None) -> np.ndarray:
        """Fit and transform in one step."""
        self.fit(X, y)
        return self.transform(X)

    def get_params_dict(self) -> dict[str, Any]:
        """Get parameters for serialization."""
        return {
            "steps": [
                (name, type(transformer).__name__) for name, transformer in self.steps
            ],
        }

    def save(self, path: Path) -> None:
        """Save pipeline to disk.

        Args:
            path: Path to save pipeline
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(self, path)

    @classmethod
    def load(cls, path: Path) -> "TransformerPipeline":
        """Load pipeline from disk.

        Args:
            path: Path to load pipeline from

        Returns:
            Loaded TransformerPipeline instance
        """
        return joblib.load(path)


def create_default_transformer_pipeline() -> TransformerPipeline:
    """Create the default transformer pipeline.

    Returns:
        TransformerPipeline with standard preprocessing steps
    """
    return TransformerPipeline(
        steps=[
            ("imputer", MissingValueHandler(numeric_strategy="median")),
            ("scaler", FeatureScaler(with_mean=True, with_std=True)),
        ]
    )
